##############################################################################
#  SEARCH KEYWORD   *** REQUIRED ***   TO SET YOUR CONFIGURATIONS
#
# Since python does not recognize '~' as home directory, please use
# EXPLICIT HOME DIRECTORY PATH instead of '~'
#
# By default, the default home directory is HOAXY_HOME='/YOUR_HOME/.hoaxy/',
# thus in the following configurations, relative paths
# 'logging.handlers.file.filename' and 'lucene.index_dir' are derived from
# HOAXY_HOME. You can set the environment variable HOAXY_HOME for your
# application. Or you can use absolute path to avoid the HOAXY_HOME directory.

###############################################################################
# Global settings
###############################################################################
#
# Many options:
#   - generate a warning message after a bucket of tweets has been recebied
#   - each time, index as much as windows_size pages
#
window_size: 1000

###############################################################################
# Logger configuration. When using multiprocessing,
# You can modify the values, but you must keep all these keyword
###############################################################################
logging:
  version: 1
  disable_existing_loggers: false
  formatters:
      default:
          format: '%(asctime)s - %(name)s - %(levelname)s: %(message)s'
  handlers:
      console:
          class: logging.StreamHandler
          formatter: default
          level: DEBUG
          stream: ext://sys.stdout
      file:
          class: logging.handlers.RotatingFileHandler
          formatter: default
          level: INFO
          # if filename is a relative path, it starts from HOAXY_HOME
          filename: 'hoaxy.log'
          # 50 MB
          maxBytes: 52428800
          encoding: 'utf-8'
          backupCount: 4
  loggers:
    scrapy:
      level: WARNING
  root:
      level: DEBUG
      handlers: [console, file]

###############################################################################
# Settings for database
# We test only on postgresql, please use POSTGRESQL>=9.3
# *** REQUIRED *** SELF CONFIGURATIONS
###############################################################################
database:
  connect_args:
    drivername: postgresql
    host: localhost
    port: 5432
    username: '*** REQUIRED ***'
    password: '*** REQUIRED ***'
    database: '*** REQUIRED ***'
  # SQLALCHEMY engine connection settings
  # http://docs.sqlalchemy.org/en/latest/core/engines.html
  pool_size: 5
  pool_recycle: -1

###############################################################################
# settings for web crawling
###############################################################################
crawl:
  # default settings for scrapy, please check references from scrapy
  # some settings, e.g., CONCURRENT_REQUESTS, CONCURRENT_REQUESTS_PER_DOMAIN
  # will be overwritten for different crawling process
  # view command modules to see the settings.
  scrapy:
    LOG_ENABLED: false
    LOG_FORMATTER: hoaxy.utils.log.PrettyLogFormatter
    # DOWNLOAD_DELAY: 0.2
    # CONCURRENT_REQUESTS: 64
    # CONCURRENT_REQUESTS_PER_DOMAIN: 4
    # USER_AGENT: 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:46.0) Gecko/20100101 Firefox/46.0'
    TELNETCONSOLE_ENABLED: false
    COOKIES_ENABLED: false    # for our experience, disable cookies is better
  article_parser:
    # Web content parser, we find two parser, one is python-goose, the other is
    # a webparser from *** https://mercury.postlight.com/ ***
    # We found that webparser works better than python-goose,
    # so we use webparser as default to parse all web news articles.
    # We use local version of mercury parse which you need to install
    # following https://www.npmjs.com/package/@postlight/mercury-parser,
    # You need node installed in your system, you can find the nodejs script for
    # parser in <hoaxy-backend>/hoaxy/node_scrips
    # *** REQUIRE *** SELF CONFIGURATION
    node_installation_path: '**** REQUIRED ****'
    parse_with_mercury_js_path: '**** REQUIRED ****'
  # Many URLs we collected are coming from domains we do not need. For these
  # urls we do not parse their html page.
  excluded_domains:
    - twitter.com
    - youtube.com
    - google.com
    - facebook.com


###############################################################################
# settings for social networks
###############################################################################
sns:
# settings for twitter stream
  twitter:
    # whether save tweet if it contains no url
    save_none_url_tweet: true
    # twitter app credentials, for twitter steaming api
    # *** REQUIRE *** SELF CONFIGURATION
    app_credentials:
      access_token: 'Please use your twitter app credentials'
      access_token_secret: '*** REQUIRED ***'
      consumer_key: ''
      consumer_secret: ''
      bearer_token: ''
    #  <twitter.url>, add comma seperated list of output parameters
    output_fields: 'attachments,'
###############################################################################
# settings for lucene indexing and searching
###############################################################################
lucene:
  # Lucene index directory.
  # if index_dir is a relative path, it starts from HOAXY_HOME
  index_dir: '*** REQUIRED ***'
  # datetime to string format, used for datetime fields
  date_format: '%Y-%m-%dT%H:%M:%S'
  # Lucene query score weight
  boost:
    title: 8.0
    canonical_url: 4.0
    meta: 2.0
    content: 1.0

##############################################################################
# Botometer
# Fetch bot score for top users
# See https://github.com/IUNetSci/botometer-python
#############################################################################
botometer:
  # if enabled is True, you must provide twitter_app_credentials
  # and rapid_key
  # rapid_key represent HTTP header 'X-RapidAPI-Key'
  # When you consuming the botometer, you need to provide this key as a way
  # to authenticate, see Consuming APIS section of  https://docs.rapidapi.com/docs/
  enabled: false
  twitter_app_credentials:
    access_token: 'Please use your twitter app credentials'
    access_token_secret: '*** REQUIRED if enabled is true ***'
    consumer_key: ''
    consumer_secret: ''
  rapid_key: '*** REQUIRED if enabled is true ***'

###############################################################################
# settings for api
###############################################################################
api:
  # for API that searching articles:
  # sort by relevance,
  #   - first get n_query_api_returned of top lucene score
  #   - then join these records with database to get tweets sharing info
  #   - finally return them by the number of tweets sharing.
  # sort by recency,
  #   - lucene search results are sorted by timestamp descending
  #   - return n_query_of_recent_sorting results
  #   - iterate over these results to collect n_query_api_returned
  #     of records with constrain that the score of records are no less than
  #     min_score_of_recent_sorting
  #   - then join these records with database to get tweets sharing info
  #   - finally return them by the number of tweets sharing.

  # number of records returned from lucene when sorted by recent
  n_query_of_recent_sorting: 1000
  # number of records returned from API (/articles)
  n_query_api_returned: 100
  # minimum score threshold used when searching is sorted by recency
  min_score_of_recent_sorting: 0.4
  # For API latest-articles:
  # Set path of the file consisting of selected fake sites
  # The format of this file is one domain per line.
  # If not provided, `claim` sites would be used for this API
  selected_fake_domains_path: selected_fake_domains.txt
  # parameters used when transforming pandas.dataframe to json
  # http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html
  dataframe_to_json_kwargs:
    orient: records
    date_format: iso
  # After searcher_refresh_interval, lucene search will update its content
  # to provide latest indexing results.
  searcher_refresh_interval:
    seconds: 3600
  # Settings for using rapid platform
  # IF YOU DO NOT WANT TO USE RAPID API PLATFORM TO PROVIDE API
  # MODIFY THE hoaxy.backend.api MODULE
  rapid:
    # the X-RapidAPI-Proxy-Secret key,
    # See https://docs.rapidapi.com/docs/headers-sent-by-mashape-proxy2
    # You should find this key in your rapid API network settings
    # For security reasons, you should protect your API and block requests
    # coming from outside the rapid api infrastructure.
    # Rapid adds the "X-RapidAPI-Proxy-Secret" header on every request.
    # This header has a unique value for every API, and if the header is
    # missing or has a different value, you can assume the request is not
    # coming from your infrastructure.
    # Please be aware the difference from rapid_key in botometer section.
    # Use 'X-RapidAPI-Proxy-Secret' when you are providing API service.
    enabled: true
    # *** REQUIRE IF ENABLED TRUE *** SELF CONFIGURATION
    secret: '*** REQUIRED ***'
    # List of IPs used by RapidAPI: https://docs.rapidapi.com/v2.1/docs/firewall-ip-security
    # (formerly RapidAPI, updated Feb 15 2019)
    ips:
      - '107.23.255.128'
      - '107.23.255.129'
      - '107.23.255.131'
      - '107.23.255.132'
      - '107.23.255.133'
      - '107.23.255.134'
      - '107.23.255.135'
      - '107.23.255.137'
      - '107.23.255.138'
      - '107.23.255.139'
      - '107.23.255.140'
      - '107.23.255.141'
      - '107.23.255.142'
      - '107.23.255.143'
      - '107.23.255.144'
      - '107.23.255.145'
      - '107.23.255.146'
      - '107.23.255.147'
      - '107.23.255.148'
      - '107.23.255.149'
      - '107.23.255.150'
      - '107.23.255.151'
      - '107.23.255.152'
      - '107.23.255.153'
      - '107.23.255.154'
      - '107.23.255.155'
      - '107.23.255.156'
      - '107.23.255.157'
      - '107.23.255.158'
      - '107.23.255.159'
      - '35.162.152.183'
      - '52.38.28.241'
      - '52.35.67.149'
      - '54.149.215.237'
      - '13.127.146.34'
      - '13.127.207.241'
      - '13.232.235.243'
      - '13.233.81.143'
      - '13.112.233.15'
      - '54.250.57.56'
      - '18.182.156.77'
      - '52.194.200.157'
      - '3.120.160.95'
      - '18.184.214.33'
      - '18.197.117.10'
      - '3.121.144.151'
      - '13.239.156.114'
      - '13.238.1.253'
      - '13.54.58.4'
      - '54.153.234.158'
      - '18.228.167.221'
      - '18.228.209.157'
      - '18.228.209.53'
      - '18.228.69.72'
      - '13.228.169.5'
      - '3.0.35.31'
      - '3.1.111.112'
      - '52.220.50.179'
      - '34.250.225.89'
      - '52.30.208.221'
      - '63.34.177.151'
      - '63.35.2.11'
      # you can add localhost for testing
      # - '127.0.0.1'

